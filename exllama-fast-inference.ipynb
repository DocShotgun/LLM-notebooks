{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## ExLlama Fast Inference\nSupports ExLlama WebUI as well as [Oobabooga WebUI API imitation](https://gist.github.com/BlankParenthesis/4f490630b6307ec441364ab64f3ce900)\n\nUp to 33B 4-bit on 2x T4, and 13B 4-bit on 1x P100 or 1x T4\n### Installation","metadata":{}},{"cell_type":"code","source":"# Kaggle\n%cd /kaggle/\n\n# Colab\n# %cd /content/\n\n# Install ExLlama and deps\n!pip install -q --pre torch --index-url https://download.pytorch.org/whl/nightly/cu118\n!pip install -q safetensors sentencepiece ninja\n!pip install -q huggingface_hub\n\n!git clone https://github.com/turboderp/exllama\n%cd exllama\n\n# Install WebUI deps\n!pip install -q flask waitress\n\n# Install deps for Oobabooga WebUI API imitation\n!wget \"https://gist.githubusercontent.com/BlankParenthesis/4f490630b6307ec441364ab64f3ce900/raw/38f4feb8ea2c023907eaacf4a98c645bca2dfe3a/api.py\"\n!pip install -q flask_sock\n\n# Install localtunnel to access Flask/API\n!npm install localtunnel","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model download\nDownload using HuggingFace repo ID","metadata":{}},{"cell_type":"code","source":"# Full repo download method\n\n# Select model\nrepo_id = \"TheBloke/Chronoboros-33B-GPTQ\"\n#repo_id = \"TheBloke/chronos-33b-GPTQ\"\n#repo_id = \"ausboss/llama-30b-supercot-4bit\"\n#repo_id = \"CalderaAI/30B-Lazarus-GPTQ4bit\"\n\n#repo_id = \"TheBloke/Llama-2-13B-GPTQ\"\n#repo_id = \"TheBloke/chronos-hermes-13B-GPTQ\"\n#repo_id = \"TehVenom/Metharme-13b-4bit-GPTQ\"\n#repo_id = \"TheBloke/Nous-Hermes-13B-GPTQ\"\n\n# Select branch\nrevision=\"main\"\n#revision=\"gptq-4bit-128g-actorder_True\"\n#revision=\"gptq-8bit-128g-actorder_True\"\n\n# Download model\nfrom huggingface_hub import snapshot_download\nsnapshot_download(repo_id=repo_id, revision=revision, local_dir=f\"./{repo_id.replace('/', '_')}\")\n\nimport os\nos.environ[\"MODEL_DIR\"] = f\"{repo_id.replace('/', '_')}\"\n\nprint(f\"Model dir: './{repo_id.replace('/', '_')}'\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Old download method - for repos where multiple versions are in the same branch\n\n# Select model\nrepo_id = \"reeducator/bluemoonrp-30b\"\nmodel_filename = \"bluemoonrp-30b-4bit-128g.safetensors\" # From the model repo\n\n# Select branch\nrevision=\"main\"\n\n# Download model\nfrom huggingface_hub import hf_hub_download\nhf_hub_download(repo_id=repo_id, revision=revision, filename=\"config.json\", local_dir=f\"./{repo_id.replace('/', '_')}\")\nhf_hub_download(repo_id=repo_id, revision=revision, filename=\"tokenizer.model\", local_dir=f\"./{repo_id.replace('/', '_')}\")\nhf_hub_download(repo_id=repo_id, revision=revision, filename=model_filename, local_dir=f\"./{repo_id.replace('/', '_')}\")\n\nimport os\nos.environ[\"MODEL_DIR\"] = f\"{repo_id.replace('/', '_')}\"\n\nprint(f\"Model dir: './{repo_id.replace('/', '_')}'\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Full repo download lora\n\n# Select model\nrepo_id = \"Ruaif/Kimiko_13B\"\n\n# Select branch\nrevision=\"main\"\n\n# Download model\nfrom huggingface_hub import snapshot_download\nsnapshot_download(repo_id=repo_id, revision=revision, local_dir=f\"./{repo_id.replace('/', '_')}\")\n\nimport os\nos.environ[\"LORA_DIR\"] = f\"{repo_id.replace('/', '_')}\"\n\nprint(f\"Lora dir: './{repo_id.replace('/', '_')}'\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Delete downloaded model\n!rm -r $MODEL_DIR\n!dir","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Run inference\nSelect either ExLlama WebUI or Oobabooga WebUI API imitation","metadata":{}},{"cell_type":"code","source":"# ExLlama WebUI\n# Access localtunnel page and input IP as password\n!curl ipv4.icanhazip.com\n\n# 2x T4\n!python ./webui/app.py -d $MODEL_DIR --host \"127.0.0.1:5000\" -gs 8,11 & npx localtunnel --port 5000\n#!python ./webui/app.py -d $MODEL_DIR --host \"127.0.0.1:5000\" -gs 8,11 -l 4096 & npx localtunnel --port 5000 # 4k context for Llama 2\n#!python ./webui/app.py -d $MODEL_DIR --lora $LORA_DIR --host \"127.0.0.1:5000\" -gs 8,11 -l 4096 & npx localtunnel --port 5000 # 4k context for Llama 2 + lora\n#!python ./webui/app.py -d $MODEL_DIR --host \"127.0.0.1:5000\" -gs 8,11 -l 4096 -a 2.5 & npx localtunnel --port 5000 # 4k NTK scaling for Llama 1\n\n# 1x P100 or 1x T4\n#!python ./webui/app.py -d $MODEL_DIR --host \"127.0.0.1:5000\" & npx localtunnel --port 5000\n#!python ./webui/app.py -d $MODEL_DIR --host \"127.0.0.1:5000\" -l 4096 & npx localtunnel --port 5000 # 4k context for Llama 2\n#!python ./webui/app.py -d $MODEL_DIR --lora $LORA_DIR --host \"127.0.0.1:5000\" -l 4096 & npx localtunnel --port 5000 # 4k context for Llama 2 + lora\n#!python ./webui/app.py -d $MODEL_DIR --host \"127.0.0.1:5000\" -l 4096 -a 2.5 & npx localtunnel --port 5000 # 4k NTK scaling for Llama 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Oobabooga WebUI API imitation\n# Access localtunnel page and input IP as password\n# Standard API: https://X/api\n# Streaming API: ws://X/api/v1/stream\n!curl ipv4.icanhazip.com\n\n# 2x T4\n!python api.py -d $MODEL_DIR -gs 8,11 & npx localtunnel --port 5000\n#!python api.py -d $MODEL_DIR -gs 8,11 -l 4096 & npx localtunnel --port 5000 # 4k context for Llama 2\n#!python api.py -d $MODEL_DIR --lora $LORA_DIR -gs 8,11 -l 4096 & npx localtunnel --port 5000 # 4k context for Llama 2 + lora\n#!python api.py -d $MODEL_DIR -gs 8,11 -l 4096 -a 2.5 & npx localtunnel --port 5000 # 4k NTK scaling for Llama 1\n\n# 1x P100 or 1x T4\n#!python api.py -d $MODEL_DIR & npx localtunnel --port 5000\n#!python api.py -d $MODEL_DIR -l 4096 & npx localtunnel --port 5000 # 4k context for Llama 2\n#!python api.py -d $MODEL_DIR --lora $LORA_DIR -l 4096 & npx localtunnel --port 5000 # 4k context for Llama 2 + lora\n#!python api.py -d $MODEL_DIR -l 4096 -a 2.5 & npx localtunnel --port 5000 # 4k NTK scaling for Llama 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Misc tests","metadata":{}},{"cell_type":"code","source":"# Benchmarking speeds\n\n# 2x T4\n!python test_benchmark_inference.py -d $MODEL_DIR -p -gs 8,11\n\n# 1x P100 or 1x T4\n#!python test_benchmark_inference.py -d $MODEL_DIR -p","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Benchmarking perplexity\n\n# 2x T4\n!python test_benchmark_inference.py -d $MODEL_DIR -ppl -ppl_ds \"./datasets/wikitext2_val_sample.jsonl\" -gs 8,11\n\n# 1x P100 or 1x T4\n#!python test_benchmark_inference.py -d $MODEL_DIR -ppl -ppl_ds \"./datasets/wikitext2_val_sample.jsonl\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}