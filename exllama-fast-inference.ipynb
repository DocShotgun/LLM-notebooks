{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## ExLlama Fast Inference\nSupports ExLlama WebUI as well as [Oobabooga WebUI API imitation](https://gist.github.com/BlankParenthesis/4f490630b6307ec441364ab64f3ce900)\n\nUp to 34B 4-bit on 2x T4, and 13B 4-bit on 1x P100 or 1x T4\n### Installation","metadata":{}},{"cell_type":"code","source":"# Kaggle\n%cd /kaggle/\n\n# Colab\n# %cd /content/\n\n# Install ExLlama and deps\n!pip install -q --pre torch --index-url https://download.pytorch.org/whl/nightly/cu118\n!pip install -q safetensors sentencepiece ninja\n!pip install -q huggingface_hub\n\n!git clone https://github.com/turboderp/exllama\n%cd exllama\n\n# Install WebUI deps\n!pip install -q flask waitress\n\n# Install deps for Oobabooga WebUI API imitation\n!wget \"https://gist.githubusercontent.com/BlankParenthesis/4f490630b6307ec441364ab64f3ce900/raw/38f4feb8ea2c023907eaacf4a98c645bca2dfe3a/api.py\"\n!pip install -q flask_sock\n\n# Install localtunnel to access Flask/API\n!npm install localtunnel","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model download\nDownload using HuggingFace repo ID","metadata":{}},{"cell_type":"code","source":"# Login to hub (to access private models)\nfrom huggingface_hub import notebook_login\nnotebook_login()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Full repo download method\n\n# Select model\nrepo_id = \"TheBloke/Chronoboros-33B-GPTQ\"\n#repo_id = \"TheBloke/chronos-33b-GPTQ\"\n#repo_id = \"ausboss/llama-30b-supercot-4bit\"\n#repo_id = \"CalderaAI/30B-Lazarus-GPTQ4bit\"\n\n#repo_id = \"TheBloke/Llama-2-13B-GPTQ\"\n#repo_id = \"TheBloke/chronos-hermes-13B-GPTQ\"\n#repo_id = \"TehVenom/Metharme-13b-4bit-GPTQ\"\n#repo_id = \"TheBloke/Nous-Hermes-13B-GPTQ\"\n\n# Select branch\n#revision=\"main\"\nrevision=\"gptq-4bit-128g-actorder_True\"\n#revision=\"gptq-8bit-128g-actorder_True\"\n\n# Download model\nfrom huggingface_hub import snapshot_download\nsnapshot_download(repo_id=repo_id, revision=revision, local_dir=f\"./{repo_id.replace('/', '_')}\")\n\nimport os\nos.environ[\"MODEL_DIR\"] = f\"{repo_id.replace('/', '_')}\"\n\nprint(f\"Model dir: './{repo_id.replace('/', '_')}'\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Old download method - for repos where multiple versions are in the same branch\n\n# Select model\nrepo_id = \"reeducator/bluemoonrp-30b\"\nmodel_filename = \"bluemoonrp-30b-4bit-128g.safetensors\" # From the model repo\n\n# Select branch\nrevision=\"main\"\n\n# Download model\nfrom huggingface_hub import hf_hub_download\nhf_hub_download(repo_id=repo_id, revision=revision, filename=\"config.json\", local_dir=f\"./{repo_id.replace('/', '_')}\")\nhf_hub_download(repo_id=repo_id, revision=revision, filename=\"tokenizer.model\", local_dir=f\"./{repo_id.replace('/', '_')}\")\nhf_hub_download(repo_id=repo_id, revision=revision, filename=model_filename, local_dir=f\"./{repo_id.replace('/', '_')}\")\n\nimport os\nos.environ[\"MODEL_DIR\"] = f\"{repo_id.replace('/', '_')}\"\n\nprint(f\"Model dir: './{repo_id.replace('/', '_')}'\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Full repo download lora\n\n# Select model\nrepo_id = \"Ruaif/Kimiko_13B\"\n\n# Select branch\nrevision=\"main\"\n\n# Download model\nfrom huggingface_hub import snapshot_download\nsnapshot_download(repo_id=repo_id, revision=revision, local_dir=f\"./{repo_id.replace('/', '_')}\")\n\nimport os\nos.environ[\"LORA_DIR\"] = f\"{repo_id.replace('/', '_')}\"\n\nprint(f\"Lora dir: './{repo_id.replace('/', '_')}'\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Delete downloaded model\n!rm -r $MODEL_DIR\n!dir","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Delete downloaded lora\n!rm -r $LORA_DIR\n!dir","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Run inference\nSelect either ExLlama WebUI or Oobabooga WebUI API imitation\n\nArguments:\n\n-d : Path to directory containing config.json, tokenizer.model and * .safetensors **(use $MODEL_DIR)**\n\n--lora : Path to directory containing adapter_config.json and adapter_model.bin **(use $LORA_DIR)**\n\n-gs : Comma-separated list of VRAM (in GB) to use per GPU device for model layers **(recommend 8,11 for 33/34B models on 2x T4, disable on 1x T4 or 1x P100)**\n\n-l : Maximum sequence length **(2048 for llama 1 models and 4096 for llama 2 models, higher for extended context models)**\n\n-a : alpha for context size extension via embedding extension **(leave at 1 for trained context, ~2.5 for 4k context on llama 1 models, 93 for codellama)**\n\n-cpe : Compression factor for positional embeddings **(set to trained value for linear rope scaling models such as superhot or llongma)**","metadata":{}},{"cell_type":"code","source":"# ExLlama WebUI\n# Access localtunnel page and input IP as password\n!curl ipv4.icanhazip.com\n!python ./webui/app.py -d $MODEL_DIR --host \"127.0.0.1:5000\" -gs 8,11 -l 4096 & npx localtunnel --port 5000","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Oobabooga WebUI API imitation\n# Access localtunnel page and input IP as password\n# Standard API: https://X/api\n# Streaming API: ws://X/api/v1/stream\n!curl ipv4.icanhazip.com\n!python api.py -d $MODEL_DIR -gs 8,11 -l 4096 & npx localtunnel --port 5000","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Misc tests","metadata":{}},{"cell_type":"code","source":"# Benchmarking speeds\n!python test_benchmark_inference.py -d $MODEL_DIR -p -gs 8,11 -l 4096","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Benchmarking perplexity\n!python test_benchmark_inference.py -d $MODEL_DIR -ppl -ppl_ds \"./datasets/wikitext2_val_sample.jsonl\" -gs 8,11 -l 4096","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}