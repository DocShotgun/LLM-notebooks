{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Quantize GGUF\nBased on TheBloke's script for ggml conversion and quantization","metadata":{}},{"cell_type":"code","source":"# Install llama.cpp\n%cd /kaggle/\n!git clone https://github.com/ggerganov/llama.cpp\n%cd /kaggle/llama.cpp\n!pip install -r requirements.txt\n!make\n%cd /kaggle/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Login to hub\nfrom huggingface_hub import notebook_login\nnotebook_login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Full repo download model\n\n# Select model\nrepo_id = \"TheBloke/Llama-2-13B-fp16\"\n\n# Select branch\nrevision=\"main\"\n\n# Download model\nfrom huggingface_hub import snapshot_download\nsnapshot_download(repo_id=repo_id, revision=revision, local_dir=f\"./{repo_id.replace('/', '_')}\")\n\nprint(f\"Model dir: './{repo_id.replace('/', '_')}'\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This step is necessary only if your base model is a standard 32000 vocab model AND the uploader accidentally kept added_tokens.json in the repo\n\n# Remove added_tokens.json\n%cd /kaggle/TheBloke_Llama-2-13B-fp16\n%rm added_tokens.json\n%cd /kaggle/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set variables\ninput_dir = \"./TheBloke_Llama-2-13B-fp16\"\nbase_model_name = \"Llama-2-13B\"\nremove_fp16 = True\n\n# Run quantize\nimport os\nimport subprocess\n\ndef quantize(model, outbase, outdir):\n    llamabase = \"/kaggle/llama.cpp\"\n    ggml_version = \"ggufv1\"\n\n    if not os.path.isdir(model):\n        raise Exception(f\"Could not find model dir at {model}\")\n\n    if not os.path.isfile(f\"{model}/config.json\"):\n        raise Exception(f\"Could not find config.json in {model}\")\n\n    os.makedirs(outdir, exist_ok=True)\n    fp16 = f\"{outdir}/{outbase}.fp16.gguf\"\n\n    print(f\"Making unquantised GGUF at {fp16}\")\n    if not os.path.isfile(fp16):\n        subprocess.run(f\"python {llamabase}/convert.py {model} --outtype f16 --outfile {fp16}\", shell=True, check=True)\n    else:\n        print(f\"Unquantised GGUF already exists at: {fp16}\")\n\n    print(\"Making quants\")\n    for type in [\"q4_K_S\", \"q5_K_M\"]:\n        outfile = f\"{outdir}/{outbase}.{type}.gguf\"\n        print(f\"Making {type} : {outfile}\")\n        subprocess.run(f\"{llamabase}/quantize {fp16} {outfile} {type}\", shell=True, check=True)\n        \n    if remove_fp16:\n        os.remove(fp16)\n\nquantize(input_dir, base_model_name, \"quantized\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set variables\nusername = \"username\"\nbase_model_name = \"Llama-2-13B\"\n\n# Push to hub\nfrom huggingface_hub import create_repo, HfApi\napi = HfApi()\n\ncreate_repo(repo_id = f\"{username}/{base_model_name}-GGUF\", private = True, repo_type = \"model\", exist_ok = True)\napi.upload_folder(\n    folder_path=\"/kaggle/quantized\",\n    repo_id=f\"{username}/{base_model_name}-GGUF\",\n    allow_patterns=f\"{base_model_name}*.gguf\"\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}