{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Basic Lora Merge\nBased on TheBloke's script for lora adapter merge","metadata":{}},{"cell_type":"code","source":"# Install reqs\n%cd /kaggle/\n!pip install -U transformers peft accelerate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Login to hub\nfrom huggingface_hub import notebook_login\nnotebook_login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Full repo download model\n\n# Select model\nrepo_id = \"TheBloke/Llama-2-13B-fp16\"\n\n# Select branch\nrevision=\"main\"\n\n# Download model\nfrom huggingface_hub import snapshot_download\nsnapshot_download(repo_id=repo_id, revision=revision, local_dir=f\"./{repo_id.replace('/', '_')}\")\n\nprint(f\"Model dir: './{repo_id.replace('/', '_')}'\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Full repo download lora\n\n# Select model\nrepo_id = \"lemonilia/limarp-llama2\"\n\n# Select branch\nrevision=\"main\"\n\n# Download model\nfrom huggingface_hub import snapshot_download\nsnapshot_download(repo_id=repo_id, revision=revision, local_dir=f\"./{repo_id.replace('/', '_')}\")\n\nprint(f\"Lora dir: './{repo_id.replace('/', '_')}'\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set variables\nmodel_dir = \"./TheBloke_Llama-2-13B-fp16\"\nlora_dir = \"./lemonilia_limarp-llama2/LIMARP-Llama2-LoRA-adapter-13B\"\n\n# Push to hub vs save files\nrepo_name = \"Limarp-Merged-L2-13b\"\npush_to_hub = False\n\noutput_dir = \"merge\"\n\n# Run merge\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\nimport torch\nimport shutil\n\nimport os\n\ndef merge_lora(base_model_path, lora_path, do_push):\n    offload_model_path = \"./offload\"\n    offload_peft_path = \"./offload_peft\"\n    shutil.rmtree(offload_model_path, ignore_errors=True)\n    shutil.rmtree(offload_peft_path, ignore_errors=True)\n    os.makedirs(offload_model_path, exist_ok=True)\n    os.makedirs(offload_peft_path, exist_ok=True)\n    \n    device_map = \"cpu\"\n    float_type = torch.float16\n    \n    base_model = AutoModelForCausalLM.from_pretrained(\n        base_model_path,\n        return_dict=True,\n        torch_dtype=float_type,\n        device_map = device_map,\n        offload_folder=offload_model_path,\n        low_cpu_mem_usage=True\n    )\n\n    print(f\"Loading PEFT: {lora_path}\")\n    model = PeftModel.from_pretrained(base_model, lora_path, torch_dtype=float_type, device_map = device_map, offload_folder=offload_peft_path, low_cpu_mem_usage=True)\n    print(f\"Running merge_and_unload\")\n    model = model.merge_and_unload()\n\n    tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n    \n    if do_push:\n        model.push_to_hub(repo_name, private=True)\n        tokenizer.push_to_hub(repo_name, private=True)\n    else:\n        os.makedirs(output_dir, exist_ok=True)\n        model.save_pretrained(output_dir)\n        tokenizer.save_pretrained(output_dir)\n        print(f\"Model saved to {output_dir}\")\n    \nmerge_lora(model_dir, lora_dir, push_to_hub)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}