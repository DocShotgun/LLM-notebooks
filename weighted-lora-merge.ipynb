{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Weighted Lora Merge\nBased on implementation from https://github.com/CoffeeVampir3/ez-trainer and CLI adaption from https://github.com/zarakiquemparte/zaraki-tools","metadata":{}},{"cell_type":"code","source":"# Install reqs\n%cd /kaggle/\n!pip install git+https://github.com/huggingface/transformers peft","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Login to hub\nfrom huggingface_hub import notebook_login\nnotebook_login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Full repo download model\n\n# Select model\nrepo_id = \"TheBloke/Llama-2-13B-fp16\"\n\n# Select branch\nrevision=\"main\"\n\n# Download model\nfrom huggingface_hub import snapshot_download\nsnapshot_download(repo_id=repo_id, revision=revision, local_dir=f\"./{repo_id.replace('/', '_')}\")\n\nprint(f\"Model dir: './{repo_id.replace('/', '_')}'\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Full repo download lora\n\n# Select model\nrepo_id = \"lemonilia/limarp-llama2\"\n\n# Select branch\nrevision=\"main\"\n\n# Download model\nfrom huggingface_hub import snapshot_download\nsnapshot_download(repo_id=repo_id, revision=revision, local_dir=f\"./{repo_id.replace('/', '_')}\")\n\nprint(f\"Lora dir: './{repo_id.replace('/', '_')}'\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set variables\nmodel_dir = \"./TheBloke_Llama-2-13B-fp16\"\nlora_dir = \"./lemonilia_limarp-llama2/LIMARP-Llama2-LoRA-adapter-13B\"\nweight = 0.66\n\n# Push to hub vs save files\nrepo_name = \"Limarp-Merged-L2-13b\"\npush_to_hub = False\n\noutput_dir = \"merge\"\n\n# Run merge\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\nimport torch\nimport shutil\n\nimport os\n\ndef merge(base_model, lora_model, scaling, merge_weight=1.0):\n    weights_list = []\n\n    # Loop over all parameters\n    for name, param in lora_model.named_parameters():\n        # If the parameter name ends with '.weight', it's an original weight\n        if name.endswith('.weight'):\n            # Make sure it's not a lora_A or lora_B weight\n            if not any(substring in name for substring in ['lora_A', 'lora_B']):\n                # Construct the names of the corresponding lora_A and lora_B weights\n                layers = name.split('.')\n                try:\n                    layer = lora_model\n                    for item in layers[:-1]:  # We go until the penultimate item (excluding the 'weight' part)\n                        if 'lora' in item:  # Split further if lora_A or lora_B\n                            item, lora_item = item.split('_')\n                            layer = getattr(layer, item)\n                            layer = getattr(layer, lora_item)\n                        else:\n                            layer = getattr(layer, item)\n                        \n                    # Try to get lora_A and lora_B weights\n                    lora_A = getattr(layer, 'lora_A').default.weight\n                    lora_B = getattr(layer, 'lora_B').default.weight\n\n                    # Add a tuple to the list with the parameter name as the first item\n                    weights_list.append((name, param.data, lora_A, lora_B))\n\n                except AttributeError:\n                    pass\n                    #print(f\"Unable to find lora_A or lora_B weights for {name}\")\n\n    for (name,weight,a,b) in weights_list:\n        ab = b @ a\n        weight += ab * scaling * merge_weight\n        print(f\"Did thing for layer named {name}\")\n    \n    #clean lora loading trash\n    for name, module in base_model.named_modules():\n        if 'lora_A' in dir(module):\n            delattr(module, 'lora_A')\n        if 'lora_B' in dir(module):\n            delattr(module, 'lora_B')\n\ndef get_lora_scaling(lora_model):\n    r = lora_model.peft_config[\"default\"].r\n    alpha = lora_model.peft_config[\"default\"].lora_alpha\n\n    scaling = alpha/r\n    return scaling\n\ndef load_model(model_path, lora_path):\n    offload_model_path = \"./offload\"\n    offload_peft_path = \"./offload_peft\"\n    shutil.rmtree(offload_model_path, ignore_errors=True)\n    shutil.rmtree(offload_peft_path, ignore_errors=True)\n    os.makedirs(offload_model_path, exist_ok=True)\n    os.makedirs(offload_peft_path, exist_ok=True)\n\n    device_map = \"cpu\"\n    float_type = torch.float16\n\n    base_model = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    return_dict=True,\n    torch_dtype=float_type,\n    device_map = device_map,\n    offload_folder=offload_model_path,\n    low_cpu_mem_usage=True\n    )\n\n    print(f\"Loading PEFT: {lora_path}\")\n    lora_model = PeftModel.from_pretrained(base_model, lora_path, torch_dtype=float_type, device_map = device_map, offload_folder=offload_peft_path, low_cpu_mem_usage=True)\n    \n    return base_model, lora_model\n\ndef initiate_model_lora_merge(model_path, lora_path, merge_weight, do_push = False):\n    print(model_path)\n    print(lora_path)\n\n    base_model, lora_model = load_model(model_path, lora_path)\n    scaling = get_lora_scaling(lora_model)\n    \n    print(f\"Lora Scaling: {scaling}\")\n    \n    merge(base_model, lora_model, scaling, merge_weight=merge_weight)\n    \n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    \n    if do_push:\n        base_model.push_to_hub(repo_name, private=True)\n        tokenizer.push_to_hub(repo_name, private=True)\n    else:\n        os.makedirs(output_dir, exist_ok=True)\n        base_model.save_pretrained(output_dir)\n        tokenizer.save_pretrained(output_dir)\n    \n    print(\"Done merging.\")\n    return\n\ninitiate_model_lora_merge(model_dir, lora_dir, weight, push_to_hub)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}