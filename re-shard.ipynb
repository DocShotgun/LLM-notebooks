{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Re-Shard\nLoad a model and re-upload it with a different shard size and/or float type","metadata":{}},{"cell_type":"code","source":"# Install reqs\n%cd /kaggle/\n!pip install -U transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Login to hub\nfrom huggingface_hub import notebook_login\nnotebook_login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Full repo download model\n\n# Select model\nrepo_id = \"TheBloke/Llama-2-13B-fp16\"\n\n# Select branch\nrevision=\"main\"\n\n# Download model\nfrom huggingface_hub import snapshot_download\nsnapshot_download(repo_id=repo_id, revision=revision, local_dir=f\"./{repo_id.replace('/', '_')}\")\n\nprint(f\"Model dir: './{repo_id.replace('/', '_')}'\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set variables\nmodel_path = \"./TheBloke_Llama-2-13B-fp16\"\nrepo_name = \"Llama-2-13B\"\nfloat_type = torch.float16\nshard_size = \"2GB\"\n\n# Re-shard and push to hub\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ndevice_map = \"cpu\"\nmodel = AutoModelForCausalLM.from_pretrained(\n        model_path,\n        return_dict=True,\n        torch_dtype=float_type,\n        device_map=device_map\n    )\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel.push_to_hub(repo_name, private=True, max_shard_size=shard_size)\ntokenizer.push_to_hub(repo_name, private=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}